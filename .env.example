# =============================================================================
# Shannon Platform Environment Configuration
# =============================================================================
# This file contains all environment variables used by the Shannon platform.
# Copy this file to .env and configure according to your needs.
#
# Variable Categories:
# - üî¥ REQUIRED: Must be set for basic functionality
# - üü° RECOMMENDED: Should be set for full features
# - üü¢ OPTIONAL: Can be configured for specific use cases
# =============================================================================

# =============================================================================
# CORE LLM CONFIGURATION (üî¥ REQUIRED - At least one provider)
# =============================================================================
# You must configure at least one LLM provider for the system to function.
# The system will automatically detect and use configured providers.

# OpenAI (Most commonly used, best compatibility)
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (Claude models)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google (Gemini models for LLM)
# Note: This is for Google's Gemini LLM API, NOT for Google Custom Search
# GOOGLE_LLM_API_KEY=your_gemini_api_key_here  # Future: Consider renaming to avoid confusion
GOOGLE_API_KEY=your_gemini_api_key_here

# =============================================================================
# ADDITIONAL LLM PROVIDERS (üü¢ OPTIONAL)
# =============================================================================
# Configure these for access to additional models and providers

# Groq (Fast inference)
GROQ_API_KEY=your_groq_api_key_here

# Mistral
MISTRAL_API_KEY=your_mistral_api_key_here

# DeepInfra (Multiple open models)
DEEPINFRA_API_KEY=your_deepinfra_api_key_here

# DeepSeek (Chinese LLM provider)
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Qwen (Alibaba's LLM)
QWEN_API_KEY=your_qwen_api_key_here

# =============================================================================
# MODEL SELECTION (üü° RECOMMENDED)
# =============================================================================
# Specify which models to use for different tasks
# If not set, defaults will be used based on available providers
# Docs: see docs/providers-models.md for model routing, tiers, and overrides
# Advanced: set MODELS_CONFIG_PATH to a YAML file to define explicit model mappings

# Model for complexity analysis (smaller, faster model recommended)
# Examples: gpt-4o-mini, claude-3-5-haiku-latest, gemini-1.5-flash-latest
COMPLEXITY_MODEL_ID=gpt-4o-mini

# Model for task decomposition (more capable model recommended)
# Examples: gpt-4o, claude-3-5-sonnet-latest, gemini-1.5-pro-latest
DECOMPOSITION_MODEL_ID=claude-sonnet-4-20250514

# Path to custom models configuration file (for advanced users)
# MODELS_CONFIG_PATH=/app/config/models.yaml

# =============================================================================
# WEB SEARCH CONFIGURATION (üü° RECOMMENDED)
# =============================================================================
# Configure at least one search provider for web search functionality

# Choose your primary provider: google|serper|bing|exa|firecrawl
WEB_SEARCH_PROVIDER=google

# Google Custom Search (https://developers.google.com/custom-search)
# Free tier available, widely used
# ‚ö†Ô∏è IMPORTANT: This requires a Google Custom Search API key, NOT a Gemini API key!
# These are different services with different API keys
GOOGLE_API_KEY=your_google_cse_api_key_here          # CSE API key (NOT Gemini)
GOOGLE_SEARCH_ENGINE_ID=your_google_search_engine_id_here

# Serper API (https://serper.dev)
# Simple, affordable Google search results
SERPER_API_KEY=your_serper_api_key_here

# Bing Search API (Azure Cognitive Services)
# Enterprise-grade search from Microsoft
BING_API_KEY=your_bing_api_key_here

# Exa API (https://exa.ai)
# AI-optimized semantic neural search
EXA_API_KEY=your_exa_api_key_here

# Firecrawl API (https://firecrawl.dev)
# Web search with integrated scraping capabilities
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# =============================================================================
# DATABASE CONFIGURATION (üü¢ OPTIONAL - Has working defaults)
# =============================================================================
# Default values work for Docker Compose setup
# Only change if using external databases

# PostgreSQL
POSTGRES_USER=shannon
POSTGRES_PASSWORD=shannon
POSTGRES_DB=shannon

# Redis
REDIS_PASSWORD=                                      # Leave empty for no auth (dev only)

# Qdrant Vector Database
# EMBEDDING_DIM=1536                                 # OpenAI ada-002 dimension (default)

# =============================================================================
# SERVICE CONFIGURATION (üü¢ OPTIONAL - Has working defaults)
# =============================================================================
# Internal service URLs and ports
# Usually no need to change these

# Temporal Workflow Engine
# Format: host:port (e.g., temporal:7233)
TEMPORAL_HOST=temporal:7233

# Internal Service URLs
LLM_SERVICE_URL=http://llm-service:8000
# AGENT_CORE_URL=agent-core:50051              # Currently unused, kept for future use

# =============================================================================
# WORKFLOW & EXECUTION SETTINGS (üü° RECOMMENDED)
# =============================================================================

# Workflow Optimization
WORKFLOW_SYNTH_BYPASS_SINGLE=true                    # Skip synthesis for single results

# Agent Runtime Configuration
TOOL_PARALLELISM=5                                   # Concurrent tool executions (1=sequential)
ENABLE_TOOL_SELECTION=1                              # Auto tool selection (1=enabled, 0=disabled)

# Priority Queues (for enterprise deployments)
PRIORITY_QUEUES=off                                  # Enable priority task scheduling (on/off)

# Streaming Configuration
# STREAMING_RING_CAPACITY=1000                       # Ring buffer capacity for streaming

# =============================================================================
# CONTEXT WINDOW & BUDGETS (üü° RECOMMENDED)
# =============================================================================
# Server-side history window (messages fetched before token-aware shaping)
# Clamped to [5, 200]
HISTORY_WINDOW_MESSAGES=50
# Debugging preset window (used when context.use_case_preset="debugging")
HISTORY_WINDOW_DEBUG_MESSAGES=75

# Optional per-agent/token budgets (tokens)
# If set, acts as a cap when distributing budget to agents
# TOKEN_BUDGET_PER_AGENT=100000
# TOKEN_BUDGET_PER_TASK=500000

# Compression thresholds (for testing/tuning)
# COMPRESSION_TRIGGER_RATIO=0.75   # Trigger compression when input ‚â• 75% of budget
# COMPRESSION_TARGET_RATIO=0.375   # Compress middle to ~37.5% of budget

# =============================================================================
# RATE LIMITING & CIRCUIT BREAKING (üü¢ OPTIONAL)
# =============================================================================
# Rust Agent Core enforcement settings

# Request Limits
ENFORCE_TIMEOUT_SECONDS=90                           # Per-request timeout
ENFORCE_MAX_TOKENS=32768                             # Max tokens per LLM API call (estimated)
ENFORCE_RATE_RPS=20                                  # Requests per second per key

# Circuit Breaker
ENFORCE_CB_ERROR_THRESHOLD=0.5                      # Error rate threshold (0-1)
ENFORCE_CB_WINDOW_SECONDS=30                        # Rolling window duration
ENFORCE_CB_MIN_REQUESTS=20                          # Min requests before evaluation

# Distributed Rate Limiting (Redis-backed)
# Uncomment for multi-instance deployments
# ENFORCE_RATE_REDIS_URL=redis://redis:6379
# ENFORCE_RATE_REDIS_PREFIX=rate:
# ENFORCE_RATE_REDIS_TTL=60

# Tool-Specific Rate Limits (per minute)
WEB_SEARCH_RATE_LIMIT=120                           # Web searches per minute
CALCULATOR_RATE_LIMIT=2000                          # Calculations per minute
PYTHON_EXECUTOR_RATE_LIMIT=60                       # Python executions per minute

# =============================================================================
# PYTHON WASI SANDBOX (üü¢ OPTIONAL)
# =============================================================================
# For sandboxed Python code execution
# Download interpreter: ./scripts/setup_python_wasi.sh

# Path to Python WASI interpreter
PYTHON_WASI_WASM_PATH=/opt/wasm-interpreters/python-3.11.4.wasm

# Alternative: Base64-encoded WASM (for containerized deployments)
# PYTHON_WASI_WASM_BASE64=<base64-encoded-python-wasm>

# =============================================================================
# MCP INTEGRATION (üü¢ OPTIONAL - Developer Preview)
# =============================================================================
# Model Context Protocol for external tool integration

# MCP Security & Limits
# MCP_ALLOWED_DOMAINS=localhost,127.0.0.1           # Comma-separated allowed hosts, or use wildcard * to allow all
# MCP_MAX_RESPONSE_BYTES=10485760                   # Max response size (10MB default)
# MCP_RETRIES=3                                     # Retry attempts
# MCP_TIMEOUT_SECONDS=10                            # Request timeout
# MCP_REGISTER_TOKEN=                               # Registration API protection token (used by both MCP and OpenAPI)

# MCP Tool API Keys (referenced in config/shannon.yaml)
# Add your tool-specific API keys here:
# WEATHER_API_KEY=your_weather_api_key_here
# GAODE_API_KEY=your_gaode_maps_key_here
# TRANSLATION_API_KEY=your_translation_key_here
# STOCK_API_KEY=your_stock_market_key_here

# =============================================================================
# OPENAPI INTEGRATION (üü¢ OPTIONAL - MVP Feature)
# =============================================================================
# Dynamically load OpenAPI specs as Shannon tools

# OpenAPI Security & Limits
OPENAPI_ALLOWED_DOMAINS=*                           # Comma-separated allowed hosts, or use wildcard * to allow all (dev)
OPENAPI_MAX_SPEC_SIZE=5242880                       # Max spec size in bytes (5MB default)
OPENAPI_FETCH_TIMEOUT=30                            # Spec fetch timeout in seconds

# OpenAPI Tool API Keys (referenced in config/shannon.yaml)
# Add your API-specific keys here - use $ prefix in YAML to reference:
# PETSTORE_API_KEY=your_petstore_key_here
# GITHUB_TOKEN=your_github_token_here
# OPENWEATHER_API_KEY=your_openweather_key_here
# API_USERNAME=your_api_username
# API_PASSWORD=your_api_password

# =============================================================================
# HUMAN-IN-THE-LOOP APPROVAL (üü¢ OPTIONAL)
# =============================================================================
# Enable human approval for high-risk or complex tasks

# Enable approval workflow
APPROVAL_ENABLED=false

# Complexity threshold for triggering approval (0.0-1.0)
# Tasks with complexity >= this value will require approval
APPROVAL_COMPLEXITY_THRESHOLD=0.5

# Dangerous tools that always require approval (comma-separated)
# Common values: file_system,code_execution,shell_command
APPROVAL_DANGEROUS_TOOLS=file_system,code_execution

# Approval timeout in seconds (default: 7200 = 2 hours)
APPROVAL_TIMEOUT_SECONDS=7200

# =============================================================================
# GATEWAY & AUTHENTICATION (üî¥ REQUIRED for production)
# =============================================================================

# JWT Authentication Secret
# üö® IMPORTANT: Generate a secure secret for production!
# Generate with: openssl rand -hex 32
JWT_SECRET=development-only-secret-change-in-production

# Development Mode Authentication Bypass
# ‚ö†Ô∏è WARNING: Set to 0 in production!
GATEWAY_SKIP_AUTH=1                                 # 1=skip auth (dev), 0=require auth (prod)

# =============================================================================
# OBSERVABILITY (üü¢ OPTIONAL)
# =============================================================================
# OpenTelemetry configuration for distributed tracing

# OTEL_SERVICE_NAME=shannon-llm-service             # Service name for traces
# OTEL_EXPORTER_OTLP_ENDPOINT=localhost:4317        # OTLP collector endpoint (format: host:port)

# Admin Server
# ADMIN_SERVER=                                     # Base URL for orchestrator admin; used by LLM service to derive events ingest endpoint

# =============================================================================
# DEVELOPMENT & TESTING (üü¢ OPTIONAL)
# =============================================================================
# Variables used for development and testing

# Seed initial data into vector database
# SEED_DATA=false                                   # Set to true to seed Qdrant

# =============================================================================
# NOTES
# =============================================================================
# 1. Environment variables can also be set at runtime or in Docker Compose
# 2. Variables in this file override defaults but are overridden by runtime env
# 3. For production deployments, use a secrets management system
# 4. Check docs/configuration.md for detailed configuration guide
# =============================================================================
