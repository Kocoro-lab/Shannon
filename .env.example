# Shannon Platform Environment Configuration

# LLM Provider API Keys
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GROQ_API_KEY=your_groq_api_key_here
GOOGLE_API_KEY=your_google_api_key_here
MISTRAL_API_KEY=your_mistral_api_key_here
DEEPINFRA_API_KEY=your_deepinfra_api_key_here

COMPLEXITY_MODEL_ID=gpt-4o-mini
DECOMPOSITION_MODEL_ID=gpt-4o

# Web Search Configuration
# Choose provider: exa or firecrawl (default: exa)
WEB_SEARCH_PROVIDER=exa

# Exa API (https://exa.ai) - Recommended
# Neural search with semantic understanding
EXA_API_KEY=your_exa_api_key_here

# Firecrawl API (https://firecrawl.dev)
# Web search with integrated scraping
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# Database Configuration (usually not changed)
POSTGRES_USER=shannon
POSTGRES_PASSWORD=shannon
POSTGRES_DB=shannon

# Temporal Configuration
TEMPORAL_HOST=temporal
TEMPORAL_PORT=7233

# Service URLs (for development/testing)
LLM_SERVICE_URL=http://llm-service:8000
AGENT_CORE_URL=agent-core:50051

# Workflow orchestration
# Bypass synthesis when there's only one successful result
WORKFLOW_SYNTH_BYPASS_SINGLE=true

# Rust Agent Core Enforcement Overrides
# Per-request timeout in seconds (default 30)
ENFORCE_TIMEOUT_SECONDS=30
# Max tokens per request (estimated; default 4096)
ENFORCE_MAX_TOKENS=4096
# Rate limit per key (requests per second; default 10)
ENFORCE_RATE_RPS=10
# Circuit breaker settings
# Open circuit if error rate exceeds this threshold in the rolling window (default 0.5)
ENFORCE_CB_ERROR_THRESHOLD=0.5
# Rolling window in seconds (default 30)
ENFORCE_CB_WINDOW_SECONDS=30
# Minimum requests before evaluating error rate (default 20)
ENFORCE_CB_MIN_REQUESTS=20
# Optional: Redis-backed distributed rate limiting (per-key global RPS)
# When set, Rust uses Redis for the rate limiter; otherwise memory-only per instance
ENFORCE_RATE_REDIS_URL=redis://redis:6379
ENFORCE_RATE_REDIS_PREFIX=rate:
ENFORCE_RATE_REDIS_TTL=60

# Agent Runtime
# Number of concurrent tool executions (1=sequential, >1=parallel)
TOOL_PARALLELISM=3
# Enable automatic tool selection via LLM service (1=enabled, 0=disabled)
ENABLE_TOOL_SELECTION=1

# Priority Queue Configuration
# Enable priority-based task queues (on/off)
PRIORITY_QUEUES=off

# ==========================================
# MCP Integration (Developer Preview)
# ==========================================
# MCP_ALLOWED_DOMAINS=localhost,127.0.0.1  # Comma-separated allowed MCP hosts
# MCP_MAX_RESPONSE_BYTES=10485760          # Max response size (10MB default)
# MCP_RETRIES=3                            # Retry attempts with exponential backoff
# MCP_TIMEOUT_SECONDS=10                   # Request timeout per MCP call
# MCP_REGISTER_TOKEN=                      # Optional token to protect registration API

# MCP Tool API Keys (used in config/shannon.yaml via ${VAR_NAME})
# Examples - uncomment and add your keys:
# WEATHER_API_KEY=your_weather_api_key_here
# GAODE_API_KEY=your_gaode_maps_key_here
# TRANSLATION_API_KEY=your_translation_key_here
# STOCK_API_KEY=your_stock_market_key_here

# ==========================================
# WASI Python Interpreter Configuration
# ==========================================
# Path to Python WASI interpreter for sandboxed Python execution
# Download with: ./scripts/setup_python_wasi.sh
# Or manually from: https://github.com/vmware-labs/webassembly-language-runtimes/releases
PYTHON_WASI_WASM_PATH=/opt/wasm-interpreters/python-3.11.4.wasm

# Alternative: Use base64-encoded WASM (for smaller deployments)
# PYTHON_WASI_WASM_BASE64=<base64-encoded-python-wasm>
