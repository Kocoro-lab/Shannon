# =============================================================================
# Shannon Platform Environment Configuration
# =============================================================================
# This file contains all environment variables used by the Shannon platform.
# Copy this file to .env and configure according to your needs.
#
# Variable Categories:
# - 游댮 REQUIRED: Must be set for basic functionality
# - 游리 RECOMMENDED: Should be set for full features
# - 游릭 OPTIONAL: Can be configured for specific use cases
# =============================================================================

# =============================================================================
# CORE LLM CONFIGURATION (游댮 REQUIRED - At least one provider)
# =============================================================================
# You must configure at least one LLM provider for the system to function.
# The system will automatically detect and use configured providers.

# OpenAI (Most commonly used, best compatibility)
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (Claude models)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google (Gemini models)
GOOGLE_API_KEY=your_google_api_key_here

# =============================================================================
# ADDITIONAL LLM PROVIDERS (游릭 OPTIONAL)
# =============================================================================
# Configure these for access to additional models and providers

# Groq (Fast inference)
GROQ_API_KEY=your_groq_api_key_here

# Mistral
MISTRAL_API_KEY=your_mistral_api_key_here

# DeepInfra (Multiple open models)
DEEPINFRA_API_KEY=your_deepinfra_api_key_here

# DeepSeek (Chinese LLM provider)
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Qwen (Alibaba's LLM)
QWEN_API_KEY=your_qwen_api_key_here

# =============================================================================
# MODEL SELECTION (游리 RECOMMENDED)
# =============================================================================
# Specify which models to use for different tasks
# If not set, defaults will be used based on available providers

# Model for complexity analysis (smaller, faster model recommended)
COMPLEXITY_MODEL_ID=claude-sonnet-4-20250514

# Model for task decomposition (more capable model recommended)
DECOMPOSITION_MODEL_ID=claude-opus-4-1-20250805

# Path to custom models configuration file (for advanced users)
# MODELS_CONFIG_PATH=/app/config/models.yaml

# =============================================================================
# WEB SEARCH CONFIGURATION (游리 RECOMMENDED)
# =============================================================================
# Configure at least one search provider for web search functionality

# Choose your primary provider: google|serper|bing|exa|firecrawl
WEB_SEARCH_PROVIDER=google

# Google Custom Search (https://developers.google.com/custom-search)
# Free tier available, widely used
GOOGLE_API_KEY=your_google_api_key_here              # Can reuse from LLM section
GOOGLE_SEARCH_ENGINE_ID=your_google_search_engine_id_here

# Serper API (https://serper.dev)
# Simple, affordable Google search results
SERPER_API_KEY=your_serper_api_key_here

# Bing Search API (Azure Cognitive Services)
# Enterprise-grade search from Microsoft
BING_API_KEY=your_bing_api_key_here

# Exa API (https://exa.ai)
# AI-optimized semantic neural search
EXA_API_KEY=your_exa_api_key_here

# Firecrawl API (https://firecrawl.dev)
# Web search with integrated scraping capabilities
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# =============================================================================
# DATABASE CONFIGURATION (游릭 OPTIONAL - Has working defaults)
# =============================================================================
# Default values work for Docker Compose setup
# Only change if using external databases

# PostgreSQL
POSTGRES_USER=shannon
POSTGRES_PASSWORD=shannon
POSTGRES_DB=shannon

# Redis
REDIS_PASSWORD=                                      # Leave empty for no auth (dev only)

# Qdrant Vector Database
# EMBEDDING_DIM=1536                                 # OpenAI ada-002 dimension (default)

# =============================================================================
# SERVICE CONFIGURATION (游릭 OPTIONAL - Has working defaults)
# =============================================================================
# Internal service URLs and ports
# Usually no need to change these

# Temporal Workflow Engine
TEMPORAL_HOST=temporal
TEMPORAL_PORT=7233

# Internal Service URLs
LLM_SERVICE_URL=http://llm-service:8000
AGENT_CORE_URL=agent-core:50051

# =============================================================================
# WORKFLOW & EXECUTION SETTINGS (游리 RECOMMENDED)
# =============================================================================

# Workflow Optimization
WORKFLOW_SYNTH_BYPASS_SINGLE=true                    # Skip synthesis for single results

# Agent Runtime Configuration
TOOL_PARALLELISM=3                                   # Concurrent tool executions (1=sequential)
ENABLE_TOOL_SELECTION=1                              # Auto tool selection (1=enabled, 0=disabled)

# Priority Queues (for enterprise deployments)
PRIORITY_QUEUES=off                                  # Enable priority task scheduling (on/off)

# Streaming Configuration
# STREAMING_RING_CAPACITY=1000                       # Ring buffer capacity for streaming

# =============================================================================
# RATE LIMITING & CIRCUIT BREAKING (游릭 OPTIONAL)
# =============================================================================
# Rust Agent Core enforcement settings

# Request Limits
ENFORCE_TIMEOUT_SECONDS=30                           # Per-request timeout
ENFORCE_MAX_TOKENS=4096                             # Max tokens per request (estimated)
ENFORCE_RATE_RPS=10                                 # Requests per second per key

# Circuit Breaker
ENFORCE_CB_ERROR_THRESHOLD=0.5                      # Error rate threshold (0-1)
ENFORCE_CB_WINDOW_SECONDS=30                        # Rolling window duration
ENFORCE_CB_MIN_REQUESTS=20                          # Min requests before evaluation

# Distributed Rate Limiting (Redis-backed)
# Uncomment for multi-instance deployments
# ENFORCE_RATE_REDIS_URL=redis://redis:6379
# ENFORCE_RATE_REDIS_PREFIX=rate:
# ENFORCE_RATE_REDIS_TTL=60

# Tool-Specific Rate Limits (per minute)
WEB_SEARCH_RATE_LIMIT=60                            # Web searches per minute
CALCULATOR_RATE_LIMIT=1000                          # Calculations per minute
PYTHON_EXECUTOR_RATE_LIMIT=30                       # Python executions per minute

# =============================================================================
# PYTHON WASI SANDBOX (游릭 OPTIONAL)
# =============================================================================
# For sandboxed Python code execution
# Download interpreter: ./scripts/setup_python_wasi.sh

# Path to Python WASI interpreter
PYTHON_WASI_WASM_PATH=/opt/wasm-interpreters/python-3.11.4.wasm

# Alternative: Base64-encoded WASM (for containerized deployments)
# PYTHON_WASI_WASM_BASE64=<base64-encoded-python-wasm>

# =============================================================================
# MCP INTEGRATION (游릭 OPTIONAL - Developer Preview)
# =============================================================================
# Model Context Protocol for external tool integration

# MCP Security & Limits
# MCP_ALLOWED_DOMAINS=localhost,127.0.0.1           # Comma-separated allowed hosts
# MCP_MAX_RESPONSE_BYTES=10485760                   # Max response size (10MB default)
# MCP_RETRIES=3                                     # Retry attempts
# MCP_TIMEOUT_SECONDS=10                            # Request timeout
# MCP_REGISTER_TOKEN=                               # Registration API protection token

# MCP Tool API Keys (referenced in config/shannon.yaml)
# Add your tool-specific API keys here:
# WEATHER_API_KEY=your_weather_api_key_here
# GAODE_API_KEY=your_gaode_maps_key_here
# TRANSLATION_API_KEY=your_translation_key_here
# STOCK_API_KEY=your_stock_market_key_here

# =============================================================================
# GATEWAY & AUTHENTICATION (游댮 REQUIRED for production)
# =============================================================================

# JWT Authentication Secret
# 游뚿 IMPORTANT: Generate a secure secret for production!
# Generate with: openssl rand -hex 32
JWT_SECRET=development-only-secret-change-in-production

# Development Mode Authentication Bypass
# 丘멆잺 WARNING: Set to 0 in production!
GATEWAY_SKIP_AUTH=1                                 # 1=skip auth (dev), 0=require auth (prod)

# =============================================================================
# OBSERVABILITY (游릭 OPTIONAL)
# =============================================================================
# OpenTelemetry configuration for distributed tracing

# OTEL_SERVICE_NAME=shannon-llm-service             # Service name for traces
# OTEL_EXPORTER_OTLP_ENDPOINT=localhost:4317        # OTLP collector endpoint

# Admin Server
# ADMIN_SERVER=                                     # Admin panel configuration

# =============================================================================
# DEVELOPMENT & TESTING (游릭 OPTIONAL)
# =============================================================================
# Variables used for development and testing

# Seed initial data into vector database
# SEED_DATA=false                                   # Set to true to seed Qdrant

# =============================================================================
# NOTES
# =============================================================================
# 1. Environment variables can also be set at runtime or in Docker Compose
# 2. Variables in this file override defaults but are overridden by runtime env
# 3. For production deployments, use a secrets management system
# 4. Check docs/configuration.md for detailed configuration guide
# =============================================================================