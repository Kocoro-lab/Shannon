# Shannon Agent Core

High-performance Rust implementation of the Shannon agent execution layer, featuring secure sandboxing, intelligent tool orchestration, and comprehensive observability.

## Features

- ðŸš€ **Modern Rust Architecture** - 2025 best practices with zero-copy operations
- ðŸ”’ **WASI Sandboxing** - Secure WebAssembly execution environment
- ðŸ› ï¸ **Intelligent Tool System** - Discovery, caching, and orchestration
- ðŸ“Š **Full Observability** - OpenTelemetry tracing + Prometheus metrics
- ðŸ§· **Enforcement Gateway** - Timeouts, rate limits, circuit breakers
- ðŸ’¾ **Smart Caching** - LRU cache with TTL and automatic expiration
- ðŸ”— **Python Integration** - Seamless communication with LLM services
- âš¡ **High Performance** - Efficient memory management and parallel execution

## Quick Start

### Prerequisites

- Rust 1.75+
- Python 3.10+ (for LLM service integration)
- Docker (optional)

### Installation

```bash
# Clone the repository
git clone https://github.com/shannon/shannon.git
cd shannon/rust/agent-core

# Build the project
cargo build --release

# Run tests
cargo test --lib
```

### Running the Agent

```bash
# Start the agent server
cargo run --release

# Or with custom configuration
RUST_LOG=info \
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 \
MEMORY_POOL_SIZE_MB=1024 \
cargo run --release
```

The agent will start on:
- gRPC service: `localhost:50051`
- Metrics endpoint: `localhost:2113/metrics`

## Architecture

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed architecture documentation.

### Key Components

- **Enforcement Gateway**: Uniform per-request policies
- **Tool System**: Registry, discovery, caching, and execution
- **WASI Sandbox**: Secure WebAssembly runtime with resource limits
- **Memory Pool**: Pre-allocated memory with garbage collection
- **Observability**: OpenTelemetry tracing and Prometheus metrics

## API Usage

See [API.md](API.md) for complete API documentation.

### Quick Example

```rust
use shannon_agent_core::tools::{ToolExecutor, ToolCall};
use std::collections::HashMap;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Create executor
    let executor = ToolExecutor::new(None);
    
    // Execute a tool
    let call = ToolCall {
        tool_name: "calculator".to_string(),
        parameters: HashMap::from([
            ("expression".to_string(), serde_json::json!("42 + 58")),
        ]),
        call_id: Some("calc_1".to_string()),
    };
    
    let result = executor.execute_tool(&call).await?;
    println!("Result: {:?}", result.output);
    
    Ok(())
}
```

## Testing

### Unit Tests
```bash
cargo test --lib
```

### Integration Tests
```bash
# With real Python service
cd python/llm-service && python3 main.py &
cargo test --test integration_python_rust

# With mock server
./tests/run_integration_tests.sh --mock
```

### Performance Benchmarks
```bash
cargo bench
```

## Configuration

Configuration via environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `RUST_LOG` | Log level | `info` |
| `OTEL_EXPORTER_OTLP_ENDPOINT` | Tracing endpoint | `http://localhost:4317` |
| `OTEL_ENABLED` | Enable OpenTelemetry | `true` |
| `MEMORY_POOL_SIZE_MB` | Memory pool size | `512` |
| `WASI_MEMORY_LIMIT_MB` | WASI sandbox memory | `256` |
| `WASI_EXECUTION_TIMEOUT_SECONDS` | WASI timeout | `30` |
| `TOOL_CACHE_TTL_SECONDS` | Cache TTL | `300` |
| `TOOL_CACHE_MAX_SIZE` | Max cache entries | `1000` |
| `ENFORCE_TIMEOUT_SECONDS` | Per-request timeout | `30` |
| `ENFORCE_MAX_TOKENS` | Max tokens per request (estimated) | `4096` |
| `ENFORCE_RATE_RPS` | Rate limit per key (req/s) | `10` |
| `ENFORCE_CB_ERROR_THRESHOLD` | Circuit breaker threshold | `0.5` |
| `ENFORCE_CB_WINDOW_SECONDS` | Circuit breaker window | `30` |
| `ENFORCE_CB_MIN_REQUESTS` | CB minimum request count | `20` |
| `ENFORCE_RATE_REDIS_URL` | Redis URL for distributed rate limits | unset |
| `ENFORCE_RATE_REDIS_PREFIX` | Redis key prefix | `rate:` |
| `ENFORCE_RATE_REDIS_TTL` | Redis limiter TTL seconds | `60` |

## Development

### Project Structure
```
src/
â”œâ”€â”€ lib.rs              # Library root
â”œâ”€â”€ main.rs             # Binary entry point
â”œâ”€â”€ enforcement.rs      # Request enforcement
â”œâ”€â”€ tools.rs            # Tool executor
â”œâ”€â”€ tool_registry.rs    # Tool discovery
â”œâ”€â”€ tool_cache.rs       # Result caching
â”œâ”€â”€ wasi_sandbox.rs     # WASI runtime
â”œâ”€â”€ memory.rs           # Memory management
â”œâ”€â”€ config.rs           # Configuration
â”œâ”€â”€ tracing.rs          # OpenTelemetry
â”œâ”€â”€ metrics.rs          # Prometheus
â”œâ”€â”€ error.rs            # Error types
â””â”€â”€ grpc_server.rs      # gRPC service
```

### Building

```bash
# Development build
cargo build

# Release build with optimizations
cargo build --release

# Build with specific features
cargo build --features "feature_name"
```

### Code Quality

```bash
# Format code
cargo fmt

# Run linter
cargo clippy -- -D warnings

# Generate documentation
cargo doc --open

# Check dependencies
cargo audit
```

## Docker

### Build Image
```bash
docker build -t shannon-agent-core .
```

### Run Container
```bash
docker run -p 50051:50051 -p 2113:2113 \
  -e RUST_LOG=info \
  -e OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317 \
  shannon-agent-core
```

### Docker Compose
```yaml
version: '3.8'
services:
  agent-core:
    build: ./rust/agent-core
    ports:
      - "50051:50051"
      - "2113:2113"
    environment:
      - RUST_LOG=info
      - MEMORY_POOL_SIZE_MB=1024
    depends_on:
      - llm-service
      
  llm-service:
    build: ./python/llm-service
    ports:
      - "8000:8000"
```

## Monitoring

### Prometheus Metrics

Available at `http://localhost:2113/metrics`:

- `agent_tool_executions_total` - Tool execution count
- `agent_tool_execution_duration_seconds` - Execution latency
- `agent_cache_hits_total` - Cache hits
- `agent_memory_usage_bytes` - Memory usage
- `agent_active_tasks` - Active task count

### OpenTelemetry Tracing

Configure tracing exporter:
```bash
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
export OTEL_SERVICE_NAME=shannon-agent-core
```

## Performance

### Benchmarks

| Operation | Latency (p50) | Latency (p99) | Throughput |
|-----------|---------------|---------------|------------|
| Tool Discovery | 0.5ms | 2ms | 20k/sec |
| Tool Execution (cached) | 0.1ms | 0.5ms | 50k/sec |
| Tool Execution (uncached) | 50ms | 200ms | 1k/sec |
| Enforcement (gateway) | 0.05ms | 0.2ms | 100k/sec |
| WASI Execution | 10ms | 100ms | 500/sec |

### Optimization Tips

1. Enable caching for repeated tool calls
2. Use simple mode for basic queries
3. Configure appropriate memory pool size
4. Enable compression for large payloads
5. Use connection pooling for Python service

## Troubleshooting

### Common Issues

**Agent fails to start:**
```bash
# Check port availability
lsof -i :50051

# Verify configuration
RUST_LOG=debug cargo run
```

**Python service connection failed:**
```bash
# Test Python service
curl http://localhost:8000/health

# Use mock server for testing
./tests/run_integration_tests.sh --mock
```

**Memory issues:**
```bash
# Increase memory pool
export MEMORY_POOL_SIZE_MB=2048

# Monitor memory usage
curl http://localhost:2113/metrics | grep memory
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Write tests for new functionality
4. Ensure all tests pass
5. Run formatting and linting
6. Submit a pull request

See [CONTRIBUTING.md](../../CONTRIBUTING.md) for details.

## License

Copyright 2025 Shannon Project. All rights reserved.

## Performance Metrics

| Component | Metric | Value |
|-----------|--------|-------|
| Tool Discovery | Latency | <0.5ms |
| Cache Hit Rate | Efficiency | >80% typical |
| Memory Pool | Allocation | <0.1ms |
| Enforcement (gateway) | Overhead | <0.05ms |
| WASI Execution | Timeout | 30s max |

## Next Steps (Optional Enhancements)

1. **Distributed Cache**: Redis integration for multi-instance deployments
2. **Advanced Metrics**: Custom Grafana dashboards
3. **WASI Preview 2**: Component model support
4. **GPU Acceleration**: CUDA/ROCm for ML operations
5. **Multi-Region**: Geo-distributed deployment

## Summary

The Shannon Rust Agent Core has been successfully modernized to 2025 standards with:
- **Zero panics** through comprehensive error handling
- **High performance** with zero-copy operations and true LRU caching
- **Full observability** with OpenTelemetry and Prometheus
- **Secure execution** via WASI sandboxing
- **Complete testing** with 37 unit tests + integration suite
- **Professional documentation** for users and developers

**Status: PRODUCTION READY** âœ…

## Links

- [Architecture Documentation](../../docs/agent-core-architecture.md)
- [API Documentation](../../docs/agent-core-api.md)
- [Shannon Main Repository](https://github.com/shannon/shannon)
- [Python LLM Service](../../python/llm-service)
- [Go Orchestrator](../../go/orchestrator)
