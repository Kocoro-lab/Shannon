# Shannon LLM Service (Python)

The LLM Service is Shannon's AI provider gateway, managing LLM interactions, tool execution, and MCP integration with support for multiple providers and intelligent tool selection.

## âš ï¸ Important Setup Note

**Before building or running the service**, you must generate the protobuf files:

```bash
# From repository root
./scripts/generate_protos_local.sh
```

This creates the `python/llm-service/llm_service/grpc_gen` directory with protobuf v5-compatible files required for the Python WASI executor and other gRPC communication.

## ğŸ¯ Core Responsibilities

- **Multi-Provider LLM Gateway** - Unified interface for OpenAI, Anthropic, Google, AWS Bedrock, Azure, Groq
- **Tool Management** - MCP tool registration, validation, and execution
- **Intelligent Tool Selection** - Automatic tool selection based on task requirements
- **Web Search Integration** - Multiple search providers (Exa, Perplexity, Brave, DuckDuckGo)
- **Embeddings & Chunking** - Document processing for vector storage
- **Cost Tracking** - Token usage and cost calculation per provider

## ğŸ—ï¸ Architecture

```
HTTP API (:8000)
    â†“
FastAPI Application
    â”œâ”€â”€ LLM Router â†’ Provider Selection
    â”‚   â”œâ”€â”€ OpenAI (GPT-4, GPT-3.5)
    â”‚   â”œâ”€â”€ Anthropic (Claude 3)
    â”‚   â”œâ”€â”€ Google (Gemini)
    â”‚   â”œâ”€â”€ AWS Bedrock
    â”‚   â”œâ”€â”€ Azure OpenAI
    â”‚   â””â”€â”€ Groq
    â”œâ”€â”€ Tools Router â†’ MCP Integration
    â”‚   â”œâ”€â”€ Tool Registry
    â”‚   â”œâ”€â”€ Tool Validation
    â”‚   â””â”€â”€ Tool Execution
    â””â”€â”€ Search Router â†’ Web Search
        â”œâ”€â”€ Exa Search
        â”œâ”€â”€ Perplexity
        â”œâ”€â”€ Brave Search
        â””â”€â”€ DuckDuckGo
```

## ğŸ“ Project Structure

```
python/llm-service/
â”œâ”€â”€ main.py                      # FastAPI application entry
â”œâ”€â”€ Dockerfile                   # Container configuration
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ llm_service/
â”‚   â”œâ”€â”€ api/                    # API endpoints
â”‚   â”‚   â”œâ”€â”€ llm.py              # LLM completion endpoints
â”‚   â”‚   â”œâ”€â”€ tools.py            # Tool management endpoints
â”‚   â”‚   â””â”€â”€ health.py           # Health check endpoints
â”‚   â”œâ”€â”€ core/                   # Core functionality
â”‚   â”‚   â”œâ”€â”€ config.py           # Configuration management
â”‚   â”‚   â”œâ”€â”€ models.py           # Pydantic models
â”‚   â”‚   â””â”€â”€ metrics.py          # Prometheus metrics
â”‚   â”œâ”€â”€ tools/                  # Tool system
â”‚   â”‚   â”œâ”€â”€ manager.py          # Tool registry & execution
â”‚   â”‚   â”œâ”€â”€ selector.py         # Intelligent tool selection
â”‚   â”‚   â”œâ”€â”€ mcp_tools.py        # MCP tool integration
â”‚   â”‚   â””â”€â”€ builtin/            # Built-in tools
â”‚   â””â”€â”€ web_search/             # Search providers
â”‚       â”œâ”€â”€ manager.py          # Search orchestration
â”‚       â””â”€â”€ providers/          # Individual providers
â”œâ”€â”€ llm_provider/               # LLM provider implementations
â”‚   â”œâ”€â”€ base.py                # Abstract provider interface
â”‚   â”œâ”€â”€ openai_provider.py     # OpenAI implementation
â”‚   â”œâ”€â”€ anthropic_provider.py  # Anthropic implementation
â”‚   â”œâ”€â”€ google_provider.py     # Google Gemini
â”‚   â”œâ”€â”€ bedrock_provider.py    # AWS Bedrock
â”‚   â”œâ”€â”€ azure_provider.py      # Azure OpenAI
â”‚   â””â”€â”€ groq_provider.py       # Groq implementation
â”œâ”€â”€ integrations/               # External integrations
â”‚   â””â”€â”€ mcp/                   # MCP protocol implementation
â””â”€â”€ tests/                      # Test suite
    â”œâ”€â”€ test_providers.py       # Provider tests
    â”œâ”€â”€ test_tools.py           # Tool system tests
    â””â”€â”€ test_web_search.py      # Search tests
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- At least one LLM API key (OpenAI, Anthropic, etc.)
- Docker (for containerized deployment)

### Development Setup

```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set environment variables
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="..."

# Run locally
python main.py

# Service will be available at http://localhost:8000
```

### Docker Deployment

```bash
# Build image
docker build -t shannon-llm-service .

# Run with compose (recommended)
make dev  # From repository root

# Or run standalone
docker run -p 8000:8000 \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  shannon-llm-service
```

## ğŸ”Œ API Endpoints

### LLM Completion
```bash
# Generate completion
curl -X POST http://localhost:8000/llm/completion \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Hello"}],
    "temperature": 0.7
  }'
```

### Tool Management
```bash
# List available tools
curl http://localhost:8000/tools/list

# Execute a tool
curl -X POST http://localhost:8000/tools/execute \
  -H "Content-Type: application/json" \
  -d '{
    "tool_name": "calculator",
    "parameters": {"expression": "2+2"}
  }'

# Auto-select tools for task
curl -X POST http://localhost:8000/tools/select \
  -H "Content-Type: application/json" \
  -d '{
    "task": "Get weather in Beijing and convert temperature",
    "max_tools": 3
  }'
```

### Web Search
```bash
# Search the web
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "latest AI developments",
    "provider": "exa",
    "max_results": 5
  }'
```

### Health & Metrics
```bash
# Health check
curl http://localhost:8000/health

# Prometheus metrics
curl http://localhost:8000/metrics
```

## âš™ï¸ Configuration

### Environment Variables

```bash
# LLM Providers (at least one required)
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=...
GOOGLE_API_KEY=...
AWS_BEDROCK_ACCESS_KEY=...
AZURE_OPENAI_API_KEY=...
GROQ_API_KEY=...

# Search Providers (optional)
EXA_API_KEY=...
PERPLEXITY_API_KEY=...
BRAVE_API_KEY=...

# Tool Configuration
ENABLE_TOOL_SELECTION=1      # Auto-select tools
TOOL_PARALLELISM=4           # Parallel tool execution
MCP_REGISTER_TOKEN=...       # MCP registration auth

# Service Configuration
PORT=8000
LOG_LEVEL=INFO
METRICS_ENABLED=true
```

### Provider Models

Each provider supports different models. See [providers-models.md](../../docs/providers-models.md) for the complete list.

Common models:
- **OpenAI**: gpt-4, gpt-4-turbo, gpt-3.5-turbo
- **Anthropic**: claude-3-opus, claude-3-sonnet, claude-3-haiku
- **Google**: gemini-pro, gemini-pro-vision
- **Groq**: llama2-70b, mixtral-8x7b

## ğŸ§ª Testing

### Unit Tests
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=. --cov-report=html

# Run specific test file
pytest tests/test_providers.py
```

### Integration Tests
```bash
# Test LLM providers
python -m pytest tests/test_providers.py -k "test_openai"

# Test tool system
python -m pytest tests/test_tools.py

# Test web search
python -m pytest tests/test_web_search.py
```

### Manual Testing
```bash
# Test tool execution
curl -X POST http://localhost:8000/tools/execute \
  -d '{"tool_name": "calculator", "parameters": {"expression": "2+2"}}'

# Test LLM completion
curl -X POST http://localhost:8000/llm/completion \
  -d '{"provider": "openai", "model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": "Hi"}]}'
```

## ğŸ”§ Key Features

### Multi-Provider Support

The service abstracts provider differences:
- Unified API across all providers
- Automatic retry and fallback
- Provider-specific optimizations
- Cost tracking per provider

### MCP Tool Integration

Model Context Protocol support:
- Dynamic tool registration
- Tool validation and sandboxing
- Parallel tool execution
- Tool cost tracking

### Intelligent Tool Selection

Automatic tool selection based on:
- Task analysis
- Tool capabilities matching
- Cost optimization
- Execution time estimates

### Web Search Integration

Multiple search providers with:
- Result deduplication
- Source credibility scoring
- Content extraction
- Caching for efficiency

## ğŸ“Š Observability

### Metrics
- **Endpoint**: `:8000/metrics` (Prometheus format)
- LLM token usage and costs
- Tool execution counts and latency
- Provider availability and errors
- Search query performance

### Logging
- Structured JSON logging
- Request/response tracing
- Error tracking with context
- Performance profiling

### Health Checks
- `/health` - Basic health status
- `/health/ready` - Readiness probe
- `/health/live` - Liveness probe

## ğŸš¨ Common Issues

### Provider Authentication
- Ensure API keys are set in environment
- Check key format and validity
- Verify provider-specific requirements

### Tool Registration
- MCP tools require valid endpoints
- Check tool parameter schemas
- Verify MCP_REGISTER_TOKEN for dynamic registration

### Memory Usage
- Large context windows can consume memory
- Use streaming for long responses
- Monitor embedding batch sizes

## ğŸ“š Further Documentation

- [MCP Integration Guide](../../docs/mcp-integration.md)
- [Provider Models Reference](../../docs/providers-models.md)
- [Web Search Configuration](../../docs/web-search-configuration.md)
- [Main README](../../README.md)