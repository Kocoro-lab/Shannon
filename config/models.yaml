# Shannon Platform - Model Configuration
# GitHub: https://github.com/Kocoro-lab/Shannon
# Updated: September 2025 with latest AI models and pricing
#
# NOTE: Model tier selection is driven by:
#   1. Task complexity score (computed during decomposition)
#   2. Explicit tier requests from callers
#   3. Budget constraints and token limits
#
# The percentages below (50/40/10) are target distributions for reference,
# NOT enforced quotas. Actual usage depends on workload characteristics.
# Monitor real distribution via Prometheus metrics: llm_requests_total{tier="..."}

model_tiers:
  small:
    # Target allocation: 50% (not enforced - actual usage depends on complexity)
    # Fast, cost-optimized models for basic tasks without reasoning capabilities
    providers:
      - provider: openai
        model: gpt-4.1-nano
        priority: 1
      - provider: anthropic
        model: claude-3-5-haiku-20241022
        priority: 2
      - provider: xai
        model: grok-4-fast-non-reasoning
        priority: 3
      - provider: google
        model: gemini-2.5-flash-lite
        priority: 4
      - provider: deepseek
        model: deepseek-chat
        priority: 5
      - provider: qwen
        model: qwen3-4b-instruct-2507
        priority: 6
      - provider: mistral
        model: ministral-8b-instruct-2410
        priority: 7
      - provider: meta
        model: llama-3.2-3b
        priority: 8
      - provider: zai
        model: glm-4.5-flash
        priority: 9
      - provider: anthropic
        model: claude-3-haiku
        priority: 10

  medium:
    # Target allocation: 40% (not enforced - actual usage depends on complexity)
    # Standard models with good capability/cost balance, not heavy reasoning
    providers:
      - provider: openai
        model: gpt-4.1-mini
        priority: 1
      - provider: anthropic
        model: claude-sonnet-4-5-20250929
        priority: 2
      - provider: xai
        model: grok-4
        priority: 3
      - provider: google
        model: gemini-2.5-flash
        priority: 4
      - provider: meta
        model: llama-4-scout
        priority: 5
      - provider: deepseek
        model: 'deepseek-v3.2'
        priority: 6
      - provider: qwen
        model: qwen3-8b
        priority: 7
      - provider: mistral
        model: mistral-small-3.2-24b-instruct-2506
        priority: 8
      - provider: cohere
        model: command-r-plus-08-2024
        priority: 9
      - provider: zai
        model: glm-4.5-air
        priority: 10
      - provider: anthropic
        model: claude-3-sonnet
        priority: 11

  large:
    # Target allocation: 10% (not enforced - actual usage depends on complexity)
    # Heavy thinking/reasoning models for complex tasks
    providers:
      - provider: openai
        model: gpt-4.1
        priority: 1
      - provider: anthropic
        model: claude-opus-4-1-20250805
        priority: 2
      - provider: google
        model: gemini-2.5-pro
        priority: 3
      - provider: deepseek
        model: deepseek-r1
        priority: 4
      - provider: qwen
        model: qwen3-omni-30b-a3b-instruct
        priority: 5
      - provider: xai
        model: grok-4-fast-reasoning
        priority: 6
      - provider: mistral
        model: mistral-nemo-instruct-2407
        priority: 7
      - provider: meta
        model: llama-3.1-405b
        priority: 8
      - provider: zai
        model: glm-4.6
        priority: 9
      - provider: openai
        model: gpt-4-turbo
        priority: 10

# Model selection strategy
selection_strategy:
  mode: priority  # priority, round-robin, least-cost, random
  fallback_enabled: true
  max_retries: 3
  timeout_seconds: 120

# Cost controls
cost_controls:
  max_cost_per_request: 2.00
  max_tokens_per_request: 100000
  daily_budget_usd: 1000.0
  alert_threshold_percent: 90

# Prompt caching configuration
prompt_cache:
  enabled: true
  similarity_threshold: 0.95
  ttl_seconds: 3600
  max_cache_size_mb: 2048

# Provider-specific settings
provider_settings:
  openai:
    base_url: https://api.openai.com/v1
    timeout: 60
    max_retries: 3
    
  anthropic:
    base_url: https://api.anthropic.com
    timeout: 60
    max_retries: 3
    
  google:
    base_url: https://generativelanguage.googleapis.com
    timeout: 60
    max_retries: 3
    
  deepseek:
    base_url: https://api.deepseek.com/v1
    timeout: 60
    max_retries: 3
    
  qwen:
    base_url: https://dashscope.aliyuncs.com/api/v1
    timeout: 60
    max_retries: 3
    
  bedrock:
    region: us-east-1
    timeout: 90
    max_retries: 3
    
  ollama:
    base_url: http://host.docker.internal:11434/v1
    timeout: 120
    max_retries: 2
    
  xai:
    base_url: https://api.x.ai/v1
    timeout: 60
    max_retries: 3
    
  zai:
    base_url: https://api.z.ai/api/paas/v4
    timeout: 60
    max_retries: 3
    
  meta:
    base_url: https://api.together.xyz/v1  # Together AI for Llama models
    timeout: 90
    max_retries: 3
    
  mistral:
    base_url: https://api.mistral.ai/v1
    timeout: 60
    max_retries: 3

  cohere:
    base_url: https://api.cohere.ai/v1
    timeout: 60
    max_retries: 3

model_catalog:
  openai:
    gpt-4.1:
      model_id: gpt-4.1
      tier: large
      context_window: 128000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    gpt-4-turbo:
      model_id: gpt-4-turbo
      tier: large
      context_window: 128000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    gpt-4.1-mini:
      model_id: gpt-4.1-mini
      tier: medium
      context_window: 128000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    gpt-4.1-nano:
      model_id: gpt-4.1-nano
      tier: small
      context_window: 128000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: false
    gpt-4o:
      model_id: gpt-4o
      tier: large
      context_window: 128000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    gpt-4o-mini:
      model_id: gpt-4o-mini
      tier: small
      context_window: 128000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    o3:
      model_id: o3
      tier: large
      context_window: 128000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    o1:
      model_id: o1
      tier: large
      context_window: 128000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: false
    o1-mini:
      model_id: o1-mini
      tier: medium
      context_window: 128000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: false
    gpt-3.5-turbo:
      model_id: gpt-3.5-turbo
      tier: small
      context_window: 16384
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: false

  anthropic:
    claude-sonnet-4-20250514:
      model_id: claude-sonnet-4-20250514
      tier: large
      context_window: 200000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    claude-sonnet-4-5-20250929:
      model_id: claude-sonnet-4-5-20250929
      tier: medium
      context_window: 200000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    claude-3-5-sonnet-20241022:
      model_id: claude-3-5-sonnet-20241022
      tier: medium
      context_window: 200000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    claude-3-sonnet:
      model_id: claude-3-sonnet
      tier: medium
      context_window: 200000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    claude-3-5-haiku-20241022:
      model_id: claude-3-5-haiku-20241022
      tier: small
      context_window: 200000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    claude-3-haiku:
      model_id: claude-3-haiku
      tier: small
      context_window: 200000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    claude-opus-4-1-20250805:
      model_id: claude-opus-4-1-20250805
      tier: large
      context_window: 200000
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
      supports_vision: true

  google:
    gemini-2.5-pro:
      model_id: gemini-2.5-pro
      tier: large
      context_window: 2000000
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    gemini-2.5-flash:
      model_id: gemini-2.5-flash
      tier: medium
      context_window: 1000000
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    gemini-2.5-flash-lite:
      model_id: gemini-2.5-flash-lite
      tier: small
      context_window: 1000000
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    gemini-2.0-flash:
      model_id: gemini-2.0-flash
      tier: medium
      context_window: 1000000
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_vision: true
    gemini-2.0-flash-lite:
      model_id: gemini-2.0-flash-lite
      tier: small
      context_window: 1000000
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_vision: true

  xai:
    grok-4-fast-non-reasoning:
      model_id: grok-4-fast-non-reasoning
      tier: small
      context_window: 131072
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_reasoning: false
    grok-4:
      model_id: grok-4
      tier: medium
      context_window: 131072
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_reasoning: true
    grok-4-fast-reasoning:
      model_id: grok-4-fast-reasoning
      tier: large
      context_window: 131072
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_reasoning: true
    grok-beta:
      model_id: grok-beta
      tier: large
      context_window: 65536
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_reasoning: false

  zai:
    glm-4.5-flash:
      model_id: glm-4.5-flash
      tier: small
      context_window: 128000
      max_tokens: 96000
      supports_functions: true
      supports_streaming: true
      supports_reasoning: false
    glm-4.5-air:
      model_id: glm-4.5-air
      tier: medium
      context_window: 128000
      max_tokens: 96000
      supports_functions: true
      supports_streaming: true
      supports_reasoning: true
    glm-4.6:
      model_id: glm-4.6
      tier: large
      context_window: 200000
      max_tokens: 128000
      supports_functions: true
      supports_streaming: true
      supports_reasoning: true

  deepseek:
    deepseek-chat:
      model_id: deepseek-chat
      tier: small
      context_window: 32768
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
    deepseek-v3:
      model_id: deepseek-v3
      tier: medium
      context_window: 65536
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
    'deepseek-v3.2':
      model_id: 'deepseek-v3.2'
      tier: medium
      context_window: 65536
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
    deepseek-r1:
      model_id: deepseek-r1
      tier: large
      context_window: 131072
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
    deepseek-r1-lite:
      model_id: deepseek-r1-lite
      tier: medium
      context_window: 131072
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true

  qwen:
    qwen3-4b-instruct-2507:
      model_id: qwen3-4b-instruct-2507
      tier: small
      context_window: 32768
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
    qwen3-8b:
      model_id: qwen3-8b
      tier: medium
      context_window: 32768
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
    qwen3-omni-30b-a3b-instruct:
      model_id: qwen3-omni-30b-a3b-instruct
      tier: large
      context_window: 32768
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
    qwen3-next-80b-a3b-instruct:
      model_id: qwen3-next-80b-a3b-instruct
      tier: large
      context_window: 32768
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
    qwen3-vl-235b-a22b-instruct:
      model_id: qwen3-vl-235b-a22b-instruct
      tier: large
      context_window: 32768
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      supports_vision: true
      enabled: false  # disabled pending provider availability

  mistral:
    ministral-8b-instruct-2410:
      model_id: mistralai/Ministral-8B-Instruct-2410
      tier: small
      context_window: 32768
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
    mistral-small-3.2-24b-instruct-2506:
      model_id: mistralai/Mistral-Small-3.2-24B-Instruct-2506
      tier: medium
      context_window: 65536
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
    mistral-nemo-instruct-2407:
      model_id: mistralai/Mistral-Nemo-Instruct-2407
      tier: large
      context_window: 81920
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
    codestral-22b-v0.1:
      model_id: mistralai/Codestral-22B-v0.1
      tier: medium
      context_window: 32768
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true

  groq:
    llama-3.3-70b-versatile:
      model_id: llama-3.3-70b-versatile
      tier: large
      context_window: 128000
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
    llama-3.1-8b-instant:
      model_id: llama-3.1-8b-instant
      tier: small
      context_window: 131072
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true

  ollama:
    qwen2.5:0.5b:
      model_id: qwen2.5:0.5b
      tier: small
      context_window: 32768
      max_tokens: 4096
      supports_functions: true
      supports_streaming: true
    llama-4-scout-17b-16e-instruct:
      model_id: llama-4-scout-17b-16e-instruct
      tier: medium
      context_window: 131072
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      enabled: false  # disabled pending validation
    llama-4-maverick-17b-128e-instruct:
      model_id: llama-4-maverick-17b-128e-instruct
      tier: large
      context_window: 200000
      max_tokens: 8192
      supports_functions: true
      supports_streaming: true
      enabled: false  # disabled pending validation
    llama-guard-4-12b:
      model_id: llama-guard-4-12b
      tier: medium
      context_window: 8192
      max_tokens: 2048
      supports_functions: false
      supports_streaming: true

# Centralized model pricing (USD per 1K tokens)
# Updated: September 2025 with latest pricing
# Used across Go/Rust/Python for consistent cost calculation.
pricing:
  defaults:
    combined_per_1k: 0.005
  models:
    openai:
      gpt-3.5-turbo:
        input_per_1k: 0.0005
        output_per_1k: 0.0020
      gpt-4o-mini:
        input_per_1k: 0.00015
        output_per_1k: 0.00060
      gpt-4o:
        input_per_1k: 0.0050
        output_per_1k: 0.0150
      gpt-4.1:
        input_per_1k: 0.0050
        output_per_1k: 0.0150
      gpt-4.1-mini:
        input_per_1k: 0.0025
        output_per_1k: 0.0075
      gpt-4.1-nano:
        input_per_1k: 0.0010
        output_per_1k: 0.0030
      o3:
        input_per_1k: 0.0020
        output_per_1k: 0.0080
      o1:
        input_per_1k: 0.0150
        output_per_1k: 0.0600
      o1-mini:
        input_per_1k: 0.0030
        output_per_1k: 0.0120
      gpt-4-turbo:
        input_per_1k: 0.0100
        output_per_1k: 0.0300
        
    anthropic:
      claude-3-5-haiku-20241022:
        input_per_1k: 0.0008
        output_per_1k: 0.0040
      claude-3-5-sonnet-20241022:
        input_per_1k: 0.0030
        output_per_1k: 0.0150
      claude-3-haiku:
        input_per_1k: 0.0002
        output_per_1k: 0.0020
      claude-3-sonnet:
        input_per_1k: 0.0030
        output_per_1k: 0.0200
      claude-sonnet-4-20250514:
        input_per_1k: 0.0040
        output_per_1k: 0.0180
      claude-sonnet-4-5-20250929:
        input_per_1k: 0.0045
        output_per_1k: 0.0190
      claude-opus-4-1-20250805:
        input_per_1k: 0.0150
        output_per_1k: 0.0750
        
    google:
      gemini-2.5-pro:
        input_per_1k: 0.00125
        output_per_1k: 0.0100
      gemini-2.5-flash:
        input_per_1k: 0.00030
        output_per_1k: 0.0025
      gemini-2.5-flash-lite:
        input_per_1k: 0.00010
        output_per_1k: 0.00040
      gemini-2.0-flash:
        input_per_1k: 0.00010
        output_per_1k: 0.00040
      gemini-2.0-flash-lite:
        input_per_1k: 0.000075
        output_per_1k: 0.00030
        
    xai:
      grok-4-fast-non-reasoning:
        input_per_1k: 0.0002
        output_per_1k: 0.0005
      grok-4:
        input_per_1k: 0.0030
        output_per_1k: 0.0150
      grok-4-fast-reasoning:
        input_per_1k: 0.0002
        output_per_1k: 0.0010
      grok-beta:
        input_per_1k: 0.0380
        output_per_1k: 0.1140
        
    zai:
      glm-4.5-flash:
        input_per_1k: 0.0
        output_per_1k: 0.0
      glm-4.5-air:
        input_per_1k: 0.0002
        output_per_1k: 0.0011
      glm-4.6:
        input_per_1k: 0.0006
        output_per_1k: 0.0022
        
    meta:
      llama-3.2-3b:
        input_per_1k: 0.00006
        output_per_1k: 0.00006
      llama-4-scout:
        input_per_1k: 0.00018
        output_per_1k: 0.00059
      llama-3.1-405b:
        input_per_1k: 0.0035
        output_per_1k: 0.0035
      llama-3.3-70b:
        input_per_1k: 0.00054
        output_per_1k: 0.00088
        
    deepseek:
      deepseek-chat:
        input_per_1k: 0.00027
        output_per_1k: 0.0011
      deepseek-v3:
        input_per_1k: 0.00027
        output_per_1k: 0.0011
      'deepseek-v3.2':
        input_per_1k: 0.00030
        output_per_1k: 0.0012
      deepseek-r1:
        input_per_1k: 0.00055
        output_per_1k: 0.00219
      deepseek-r1-lite:
        input_per_1k: 0.00035
        output_per_1k: 0.0014
        
    qwen:
      qwen3-4b-instruct-2507:
        input_per_1k: 0.0001
        output_per_1k: 0.0003
      qwen3-8b:
        input_per_1k: 0.0004
        output_per_1k: 0.0012
      qwen3-omni-30b-a3b-instruct:
        input_per_1k: 0.0008
        output_per_1k: 0.0024
      qwen3-next-80b-a3b-instruct:
        input_per_1k: 0.0012
        output_per_1k: 0.0036
      qwen3-vl-235b-a22b-instruct:
        input_per_1k: 0.0015
        output_per_1k: 0.0045

    groq:
      llama-3.3-70b-versatile:
        input_per_1k: 0.00059
        output_per_1k: 0.00079
      llama-3.1-8b-instant:
        input_per_1k: 0.00005
        output_per_1k: 0.00010
      llama-4-scout-17b-16e-instruct:
        input_per_1k: 0.00040
        output_per_1k: 0.00120
      llama-4-maverick-17b-128e-instruct:
        input_per_1k: 0.00060
        output_per_1k: 0.00180
      llama-guard-4-12b:
        input_per_1k: 0.00005
        output_per_1k: 0.00005
        
    mistral:
      ministral-8b-instruct-2410:
        input_per_1k: 0.0001
        output_per_1k: 0.0003
      mistral-small-3.2-24b-instruct-2506:
        input_per_1k: 0.0004
        output_per_1k: 0.0020
      mistral-nemo-instruct-2407:
        input_per_1k: 0.0008
        output_per_1k: 0.0024
      codestral-22b-v0.1:
        input_per_1k: 0.0003
        output_per_1k: 0.0009
        
    cohere:
      command-r-plus-08-2024:
        input_per_1k: 0.0025
        output_per_1k: 0.0100
      command-r-08-2024:
        input_per_1k: 0.00015
        output_per_1k: 0.0006

    ollama:
      qwen2.5:0.5b:
        input_per_1k: 0.0
        output_per_1k: 0.0

# Feature flags for advanced capabilities
feature_flags:
  enable_thinking_mode: true
  enable_computer_use: true
  enable_multimodal: true
  enable_function_calling: true
  enable_batch_processing: true
  enable_prompt_caching: true

# Model capabilities matrix
model_capabilities:
  multimodal_models:
    - gpt-4.1
    - gpt-4o
    - claude-sonnet-4-5-20250929
    - claude-sonnet-4-5-20250929
    - gemini-2.5-flash
    - gemini-2.0-flash
    
  thinking_models:
    - gpt-4.1
    - o3
    - o1
    - o1-mini
    - claude-opus-4-1-20250805
    - gemini-2.5-pro
    - deepseek-r1
    - qwen3-omni-30b-a3b-instruct
    
  coding_specialists:
    - codestral-22b-v0.1
    - 'deepseek-v3.2'
    - claude-sonnet-4-5-20250929
    - claude-sonnet-4-20250514
    - o1
    - qwen3-omni-30b-a3b-instruct

  long_context_models:
    - llama-4-scout-17b-16e-instruct  # 10M tokens
    - gemini-2.5-flash  # 1M tokens
    - qwen3-omni-30b-a3b-instruct  # 262K tokens
    - claude-sonnet-4-5-20250929  # 200K tokens

# Rate limiting configuration
rate_limits:
  default_rpm: 60
  default_tpm: 100000
  tier_overrides:
    small:
      rpm: 120
      tpm: 200000
    medium:
      rpm: 60
      tpm: 100000
    large:
      rpm: 30
      tpm: 50000

# Monitoring and alerting
monitoring:
  enabled: true
  metrics_export_interval: 30
  cost_tracking: true
  performance_tracking: true
  error_tracking: true
  
alerts:
  cost_threshold_usd: 400.0
  error_rate_threshold: 0.05
  latency_threshold_ms: 30000
