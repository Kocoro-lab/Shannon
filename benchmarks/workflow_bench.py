#!/usr/bin/env python3
"""
Shannon å·¥ä½œæµæ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import time
import argparse
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
import sys
import json

# å¯¼å…¥é…ç½®
from config import (
    SIMPLE_TASK_TIMEOUT,
    DAG_SUBTASK_TIMEOUT,
    SIMULATION_DELAYS,
    DEFAULT_SIMPLE_REQUESTS,
    DEFAULT_CONCURRENCY,
    DEFAULT_DAG_REQUESTS,
    DEFAULT_DAG_SUBTASKS,
    DEFAULT_GRPC_ENDPOINT,
    DEFAULT_API_KEY,
    safe_percentile,
)

try:
    import grpc
    from google.protobuf import struct_pb2
    sys.path.insert(0, './clients/python/src')
    from shannon.pb import orchestrator_pb2, orchestrator_pb2_grpc, common_pb2
    GRPC_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Warning: gRPC imports failed: {e}")
    print("   Running in simulation mode. Install: pip install -e clients/python")
    GRPC_AVAILABLE = False

class WorkflowBenchmark:
    """å·¥ä½œæµæ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, endpoint="localhost:50052", api_key="test-key", use_simulation=False):
        self.endpoint = endpoint
        self.api_key = api_key
        self.use_simulation = use_simulation or not GRPC_AVAILABLE
        
        if not self.use_simulation:
            try:
                self.channel = grpc.insecure_channel(endpoint)
                self.client = orchestrator_pb2_grpc.OrchestratorServiceStub(self.channel)
                print(f"âœ… Connected to orchestrator at {endpoint}")
            except Exception as e:
                print(f"âš ï¸  Failed to connect: {e}. Using simulation mode.")
                self.use_simulation = True
    
    def _get_metadata(self):
        """Get gRPC metadata for authentication"""
        return [('x-api-key', self.api_key)]
        
    def run_simple_task(self, task_id):
        """è¿è¡Œç®€å•ä»»åŠ¡"""
        start = time.time()
        
        try:
            if self.use_simulation:
                # ä½¿ç”¨é…ç½®çš„æ¨¡æ‹Ÿå»¶è¿Ÿ
                time.sleep(SIMULATION_DELAYS['simple_task'])
                success = True
            else:
                # çœŸå® gRPC è°ƒç”¨
                request = orchestrator_pb2.TaskRequest(
                    query="Calculate the factorial of 20",
                    user_id="benchmark-user",
                    mode=common_pb2.EXECUTION_MODE_STANDARD
                )
                
                response = self.client.SubmitTask(
                    request,
                    metadata=self._get_metadata(),
                    timeout=SIMPLE_TASK_TIMEOUT
                )
                success = response.status == "completed"
            
            duration = time.time() - start
            return {
                "task_id": task_id,
                "duration": duration,
                "success": success
            }
        except Exception as e:
            duration = time.time() - start
            return {
                "task_id": task_id,
                "duration": duration,
                "success": False,
                "error": str(e)
            }
    
    def run_dag_workflow(self, num_subtasks, task_id):
        """è¿è¡Œ DAG å·¥ä½œæµ"""
        start = time.time()
        
        query = f"""
        Complete the following {num_subtasks} tasks:
        {chr(10).join([f'{i+1}. Calculate {i+1} * {i+1}' for i in range(num_subtasks)])}
        """
        
        try:
            if self.use_simulation:
                # ä½¿ç”¨é…ç½®çš„DAGå­ä»»åŠ¡å»¶è¿Ÿ
                time.sleep(num_subtasks * SIMULATION_DELAYS['dag_subtask'])
                success = True
            else:
                # çœŸå® gRPC è°ƒç”¨
                context = struct_pb2.Struct()
                context['workflow_type'] = 'dag'
                context['num_subtasks'] = num_subtasks
                
                request = orchestrator_pb2.TaskRequest(
                    query=query,
                    user_id="benchmark-user",
                    mode=common_pb2.EXECUTION_MODE_STANDARD,
                    context=context
                )
                
                response = self.client.SubmitTask(
                    request,
                    metadata=self._get_metadata(),
                    timeout=DAG_SUBTASK_TIMEOUT
                )
                success = response.status == "completed"
            
            duration = time.time() - start
            return {
                "task_id": task_id,
                "num_subtasks": num_subtasks,
                "duration": duration,
                "success": success
            }
        except Exception as e:
            duration = time.time() - start
            return {
                "task_id": task_id,
                "num_subtasks": num_subtasks,
                "duration": duration,
                "success": False,
                "error": str(e)
            }
    
    def benchmark_simple_tasks(self, num_requests=100, concurrency=10):
        """åŸºå‡†æµ‹è¯•ç®€å•ä»»åŠ¡"""
        print(f"\nğŸ“Š ç®€å•ä»»åŠ¡åŸºå‡†æµ‹è¯• ({num_requests} è¯·æ±‚, {concurrency} å¹¶å‘)")
        print("-" * 60)
        
        results = []
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            futures = [
                executor.submit(self.run_simple_task, i) 
                for i in range(num_requests)
            ]
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")
        
        self.print_statistics("ç®€å•ä»»åŠ¡", results)
        return results
    
    def benchmark_dag_workflows(self, num_requests=20, num_subtasks=5):
        """åŸºå‡†æµ‹è¯• DAG å·¥ä½œæµ"""
        print(f"\nğŸ“Š DAG å·¥ä½œæµåŸºå‡†æµ‹è¯• ({num_requests} è¯·æ±‚, {num_subtasks} å­ä»»åŠ¡)")
        print("-" * 60)
        
        results = []
        for i in range(num_requests):
            try:
                result = self.run_dag_workflow(num_subtasks, i)
                results.append(result)
                print(f"  å®Œæˆ {i+1}/{num_requests}")
            except Exception as e:
                print(f"âŒ å·¥ä½œæµå¤±è´¥: {e}")
        
        self.print_statistics("DAG å·¥ä½œæµ", results)
        return results
    
    def print_statistics(self, name, results):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if not results:
            print("âš ï¸  æ— ç»“æœ")
            return
        
        durations = [r["duration"] for r in results if r.get("success")]
        success_rate = len(durations) / len(results) * 100
        
        print(f"\n{name} ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {len(results)}")
        print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  å¹³å‡è€—æ—¶: {statistics.mean(durations):.3f}s")
        print(f"  ä¸­ä½æ•°: {statistics.median(durations):.3f}s")
        print(f"  æœ€å°å€¼: {min(durations):.3f}s")
        print(f"  æœ€å¤§å€¼: {max(durations):.3f}s")
        
        if len(durations) > 1:
            print(f"  æ ‡å‡†å·®: {statistics.stdev(durations):.3f}s")
        
        # ç™¾åˆ†ä½æ•°ï¼ˆä½¿ç”¨å®‰å…¨è®¡ç®—å‡½æ•°ï¼‰
        sorted_durations = sorted(durations)
        p50 = safe_percentile(sorted_durations, 0.50)
        p95 = safe_percentile(sorted_durations, 0.95)
        p99 = safe_percentile(sorted_durations, 0.99)
        
        print(f"\n  P50: {p50:.3f}s" if p50 else "\n  P50: N/A")
        print(f"  P95: {p95:.3f}s" if p95 else "  P95: N/A")
        print(f"  P99: {p99:.3f}s" if p99 else "  P99: N/A")
        
        # ååé‡
        total_time = max(durations)
        throughput = len(results) / total_time
        print(f"\n  ååé‡: {throughput:.1f} req/s")

def main():
    parser = argparse.ArgumentParser(description="Shannon å·¥ä½œæµåŸºå‡†æµ‹è¯•")
    parser.add_argument("--test", choices=["simple", "dag", "parallel"], 
                        default="simple", help="æµ‹è¯•ç±»å‹")
    parser.add_argument("--requests", type=int, default=DEFAULT_SIMPLE_REQUESTS, 
                        help=f"è¯·æ±‚æ•°é‡ï¼ˆé»˜è®¤: {DEFAULT_SIMPLE_REQUESTS}ï¼‰")
    parser.add_argument("--subtasks", type=int, default=DEFAULT_DAG_SUBTASKS, 
                        help=f"DAG å­ä»»åŠ¡æ•°ï¼ˆé»˜è®¤: {DEFAULT_DAG_SUBTASKS}ï¼‰")
    parser.add_argument("--concurrency", type=int, default=DEFAULT_CONCURRENCY, 
                        help=f"å¹¶å‘æ•°ï¼ˆé»˜è®¤: {DEFAULT_CONCURRENCY}ï¼‰")
    parser.add_argument("--endpoint", default=DEFAULT_GRPC_ENDPOINT, 
                        help=f"gRPC ç«¯ç‚¹ï¼ˆé»˜è®¤: {DEFAULT_GRPC_ENDPOINT}ï¼‰")
    parser.add_argument("--api-key", default=DEFAULT_API_KEY, 
                        help=f"API Keyï¼ˆé»˜è®¤: {DEFAULT_API_KEY}ï¼‰")
    parser.add_argument("--simulate", action="store_true",
                        help="ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    parser.add_argument("--output", type=str,
                        help="JSON è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    bench = WorkflowBenchmark(
        endpoint=args.endpoint,
        api_key=args.api_key,
        use_simulation=args.simulate
    )
    
    results = None
    if args.test == "simple":
        results = bench.benchmark_simple_tasks(args.requests, args.concurrency)
    elif args.test == "dag":
        results = bench.benchmark_dag_workflows(args.requests, args.subtasks)
    elif args.test == "parallel":
        print("å¹¶è¡Œæµ‹è¯•å°šæœªå®ç°")
    
    if args.output and results:
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ° {args.output}")

if __name__ == "__main__":
    main()


