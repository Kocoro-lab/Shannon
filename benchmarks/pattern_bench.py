#!/usr/bin/env python3
"""
Shannon æ¨¡å¼æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•ä¸åŒ AI æ¨¡å¼çš„æ€§èƒ½è¡¨ç°
"""

import time
import argparse
import statistics
import json
import sys
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import grpc
    from google.protobuf import struct_pb2
    # Import generated protobuf stubs
    # Note: These should be generated from protos/orchestrator/orchestrator.proto
    sys.path.insert(0, './clients/python/src')
    from shannon.pb import orchestrator_pb2, orchestrator_pb2_grpc, common_pb2
    GRPC_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Warning: gRPC imports failed: {e}")
    print("   Running in simulation mode. Install shannon client: pip install -e clients/python")
    GRPC_AVAILABLE = False


class PatternBenchmark:
    """æ¨¡å¼æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, endpoint="localhost:50052", api_key="test-key", use_simulation=False):
        self.endpoint = endpoint
        self.api_key = api_key
        self.use_simulation = use_simulation or not GRPC_AVAILABLE
        
        if not self.use_simulation:
            try:
                self.channel = grpc.insecure_channel(endpoint)
                self.client = orchestrator_pb2_grpc.OrchestratorServiceStub(self.channel)
                print(f"âœ… Connected to orchestrator at {endpoint}")
            except Exception as e:
                print(f"âš ï¸  Failed to connect to gRPC: {e}")
                print("   Falling back to simulation mode")
                self.use_simulation = True
    
    def _get_metadata(self):
        """Get gRPC metadata for authentication"""
        return [('x-api-key', self.api_key)]
    
    def run_pattern_task(self, pattern: str, query: str, task_id: int, **kwargs) -> Dict[str, Any]:
        """è¿è¡ŒæŒ‡å®šæ¨¡å¼çš„ä»»åŠ¡"""
        start = time.time()
        
        try:
            if self.use_simulation:
                # Simulation mode - estimate based on pattern complexity
                delays = {
                    'cot': 1.5,      # Chain-of-Thought
                    'react': 2.0,    # ReAct
                    'debate': 4.5,   # Debate (multiple agents)
                    'tot': 3.5,      # Tree-of-Thoughts
                    'reflection': 2.5 # Reflection
                }
                time.sleep(delays.get(pattern, 2.0))
                success = True
                output = f"Simulated {pattern} output"
                token_usage = {'input': 1000, 'output': 500}
            else:
                # Real gRPC call
                metadata = self._get_metadata()
                
                # Build context with pattern specification
                context = struct_pb2.Struct()
                context['cognitive_strategy'] = pattern
                context.update(kwargs)
                
                request = orchestrator_pb2.TaskRequest(
                    query=query,
                    user_id="benchmark-user",
                    mode=common_pb2.EXECUTION_MODE_STANDARD,
                    context=context
                )
                
                response = self.client.SubmitTask(request, metadata=metadata, timeout=120.0)
                success = response.status == "completed"
                output = response.output[:100] if hasattr(response, 'output') else ""
                
                # Extract token usage from response
                token_usage = {
                    'input': getattr(response, 'tokens_used_input', 0),
                    'output': getattr(response, 'tokens_used_output', 0)
                }
            
            duration = time.time() - start
            
            return {
                "task_id": task_id,
                "pattern": pattern,
                "duration": duration,
                "success": success,
                "output_preview": output,
                "token_usage": token_usage,
                "total_tokens": token_usage['input'] + token_usage['output']
            }
            
        except Exception as e:
            duration = time.time() - start
            return {
                "task_id": task_id,
                "pattern": pattern,
                "duration": duration,
                "success": False,
                "error": str(e),
                "token_usage": {"input": 0, "output": 0},
                "total_tokens": 0
            }
    
    def benchmark_chain_of_thought(self, num_requests=10):
        """Chain-of-Thought æ¨¡å¼åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“Š Chain-of-Thought åŸºå‡†æµ‹è¯• ({num_requests} è¯·æ±‚)")
        print("-" * 60)
        
        query = "Calculate the result of (123 + 456) * 789 and explain your reasoning step by step."
        
        results = []
        for i in range(num_requests):
            result = self.run_pattern_task('cot', query, i)
            results.append(result)
            print(f"  å®Œæˆ {i+1}/{num_requests} - {result['duration']:.2f}s")
        
        self.print_statistics("Chain-of-Thought", results)
        return results
    
    def benchmark_react(self, num_requests=10):
        """ReAct æ¨¡å¼åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“Š ReAct åŸºå‡†æµ‹è¯• ({num_requests} è¯·æ±‚)")
        print("-" * 60)
        
        query = "Search for the current weather in San Francisco and analyze if it's good for outdoor activities."
        
        results = []
        for i in range(num_requests):
            result = self.run_pattern_task('react', query, i)
            results.append(result)
            print(f"  å®Œæˆ {i+1}/{num_requests} - {result['duration']:.2f}s")
        
        self.print_statistics("ReAct", results)
        return results
    
    def benchmark_debate(self, num_requests=5, num_agents=3):
        """Debate æ¨¡å¼åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“Š Debate æ¨¡å¼åŸºå‡†æµ‹è¯• ({num_requests} è¯·æ±‚, {num_agents} agents)")
        print("-" * 60)
        
        query = "Should AI systems be regulated by government? Discuss pros and cons."
        
        results = []
        for i in range(num_requests):
            result = self.run_pattern_task(
                'debate', 
                query, 
                i,
                num_agents=num_agents,
                debate_rounds=2
            )
            results.append(result)
            print(f"  å®Œæˆ {i+1}/{num_requests} - {result['duration']:.2f}s")
        
        self.print_statistics(f"Debate ({num_agents} agents)", results)
        return results
    
    def benchmark_tree_of_thoughts(self, num_requests=5):
        """Tree-of-Thoughts æ¨¡å¼åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“Š Tree-of-Thoughts åŸºå‡†æµ‹è¯• ({num_requests} è¯·æ±‚)")
        print("-" * 60)
        
        query = "Design an algorithm to solve the traveling salesman problem for 10 cities."
        
        results = []
        for i in range(num_requests):
            result = self.run_pattern_task(
                'tot', 
                query, 
                i,
                exploration_depth=3,
                branches_per_level=3
            )
            results.append(result)
            print(f"  å®Œæˆ {i+1}/{num_requests} - {result['duration']:.2f}s")
        
        self.print_statistics("Tree-of-Thoughts", results)
        return results
    
    def benchmark_reflection(self, num_requests=10):
        """Reflection æ¨¡å¼åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“Š Reflection åŸºå‡†æµ‹è¯• ({num_requests} è¯·æ±‚)")
        print("-" * 60)
        
        query = "Write a Python function to check if a string is a palindrome. Review and improve it."
        
        results = []
        for i in range(num_requests):
            result = self.run_pattern_task(
                'reflection', 
                query, 
                i,
                reflection_rounds=2
            )
            results.append(result)
            print(f"  å®Œæˆ {i+1}/{num_requests} - {result['duration']:.2f}s")
        
        self.print_statistics("Reflection", results)
        return results
    
    def print_statistics(self, name: str, results: List[Dict]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if not results:
            print("âš ï¸  æ— ç»“æœ")
            return
        
        successful = [r for r in results if r.get("success")]
        durations = [r["duration"] for r in successful]
        tokens = [r["total_tokens"] for r in successful]
        success_rate = len(successful) / len(results) * 100
        
        print(f"\n{name} ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {len(results)}")
        print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        
        if not durations:
            print("  âŒ æ— æˆåŠŸè¯·æ±‚")
            return
        
        print(f"\n  å»¶è¿ŸæŒ‡æ ‡:")
        print(f"    å¹³å‡è€—æ—¶: {statistics.mean(durations):.3f}s")
        print(f"    ä¸­ä½æ•°: {statistics.median(durations):.3f}s")
        print(f"    æœ€å°å€¼: {min(durations):.3f}s")
        print(f"    æœ€å¤§å€¼: {max(durations):.3f}s")
        
        if len(durations) > 1:
            print(f"    æ ‡å‡†å·®: {statistics.stdev(durations):.3f}s")
        
        # ç™¾åˆ†ä½æ•°
        sorted_durations = sorted(durations)
        p50 = sorted_durations[len(sorted_durations) // 2]
        p95 = sorted_durations[min(int(len(sorted_durations) * 0.95), len(sorted_durations) - 1)]
        p99 = sorted_durations[min(int(len(sorted_durations) * 0.99), len(sorted_durations) - 1)]
        
        print(f"\n  ç™¾åˆ†ä½æ•°:")
        print(f"    P50: {p50:.3f}s")
        print(f"    P95: {p95:.3f}s")
        print(f"    P99: {p99:.3f}s")
        
        # Token ä½¿ç”¨æƒ…å†µ
        if tokens:
            print(f"\n  Token ä½¿ç”¨:")
            print(f"    å¹³å‡: {statistics.mean(tokens):.0f} tokens")
            print(f"    æ€»è®¡: {sum(tokens)} tokens")
            
            # ä¼°ç®—æˆæœ¬ (å‡è®¾ $0.01 per 1K tokens)
            total_cost = sum(tokens) * 0.00001
            print(f"    ä¼°ç®—æˆæœ¬: ${total_cost:.4f}")
    
    def run_comparison(self, requests_per_pattern=5):
        """å¯¹æ¯”æ‰€æœ‰æ¨¡å¼çš„æ€§èƒ½"""
        print("\n" + "=" * 60)
        print("æ¨¡å¼æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("=" * 60)
        
        all_results = {}
        
        # Run all pattern benchmarks
        patterns = [
            ('cot', self.benchmark_chain_of_thought),
            ('react', self.benchmark_react),
            ('debate', lambda n: self.benchmark_debate(n, 3)),
            ('tot', self.benchmark_tree_of_thoughts),
            ('reflection', self.benchmark_reflection)
        ]
        
        for pattern_name, benchmark_func in patterns:
            try:
                results = benchmark_func(requests_per_pattern)
                all_results[pattern_name] = results
            except Exception as e:
                print(f"âŒ {pattern_name} æµ‹è¯•å¤±è´¥: {e}")
        
        # Print comparison summary
        print("\n" + "=" * 60)
        print("æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        print("=" * 60)
        print(f"\n{'æ¨¡å¼':<20} {'å¹³å‡å»¶è¿Ÿ':<12} {'P95':<12} {'å¹³å‡Tokens':<15} {'æˆåŠŸç‡'}")
        print("-" * 60)
        
        for pattern_name, results in all_results.items():
            successful = [r for r in results if r.get("success")]
            if not successful:
                continue
            
            durations = [r["duration"] for r in successful]
            tokens = [r["total_tokens"] for r in successful]
            success_rate = len(successful) / len(results) * 100
            
            sorted_durations = sorted(durations)
            p95 = sorted_durations[min(int(len(sorted_durations) * 0.95), len(sorted_durations) - 1)]
            
            print(f"{pattern_name:<20} {statistics.mean(durations):>10.2f}s  "
                  f"{p95:>10.2f}s  {statistics.mean(tokens):>13.0f}  "
                  f"{success_rate:>6.1f}%")
        
        return all_results


def main():
    parser = argparse.ArgumentParser(description="Shannon æ¨¡å¼æ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--pattern", 
                        choices=["cot", "react", "debate", "tot", "reflection", "all"], 
                        default="all", 
                        help="æµ‹è¯•æ¨¡å¼")
    parser.add_argument("--requests", type=int, default=5, 
                        help="æ¯ä¸ªæ¨¡å¼çš„è¯·æ±‚æ•°")
    parser.add_argument("--agents", type=int, default=3, 
                        help="Debate æ¨¡å¼çš„ agent æ•°é‡")
    parser.add_argument("--endpoint", default="localhost:50052", 
                        help="gRPC ç«¯ç‚¹")
    parser.add_argument("--api-key", default="test-key", 
                        help="API Key")
    parser.add_argument("--simulate", action="store_true",
                        help="ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆä¸è¿æ¥çœŸå®æœåŠ¡ï¼‰")
    parser.add_argument("--output", type=str,
                        help="JSON è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    bench = PatternBenchmark(
        endpoint=args.endpoint, 
        api_key=args.api_key,
        use_simulation=args.simulate
    )
    
    all_results = {}
    
    if args.pattern == "all":
        all_results = bench.run_comparison(args.requests)
    elif args.pattern == "cot":
        all_results['cot'] = bench.benchmark_chain_of_thought(args.requests)
    elif args.pattern == "react":
        all_results['react'] = bench.benchmark_react(args.requests)
    elif args.pattern == "debate":
        all_results['debate'] = bench.benchmark_debate(args.requests, args.agents)
    elif args.pattern == "tot":
        all_results['tot'] = bench.benchmark_tree_of_thoughts(args.requests)
    elif args.pattern == "reflection":
        all_results['reflection'] = bench.benchmark_reflection(args.requests)
    
    # Save to JSON if output specified
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(all_results, f, indent=2)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ° {args.output}")


if __name__ == "__main__":
    main()

